AISE 26 â€¢ Week 9 Day 2 â€” Mini Model Serving Project
FastAPI â€¢ Batch Inference â€¢ Prometheus â€¢ Docker
<p align="center"> <img src="https://img.shields.io/badge/FastAPI-Model%20Serving-teal?style=for-the-badge"/> <img src="https://img.shields.io/badge/Batch-Inference-blueviolet?style=for-the-badge"/> <img src="https://img.shields.io/badge/Monitoring-Prometheus-orange?style=for-the-badge"/> <img src="https://img.shields.io/badge/Docker-Ready-success?style=for-the-badge"/> </p>

## ğŸŒŸ Project Snapshot

This mini-project shows the full journey of taking a simple ML model and serving it two different ways:

ğŸŸ¢ Real-time predictions using FastAPI

ğŸ“ Batch CSV predictions in a script

ğŸ“Š Metrics for monitoring using Prometheus

ğŸ³ Docker packaging for deployment

It's intentionally small, clean, and focused â€” just enough to demonstrate a real production flow.

## ğŸ—‚ Project Layout
```
app/
  main.py          <-- FastAPI app (predict + health + metrics)
  metrics.py       <-- Prometheus counters & histogram
models/
  baseline.joblib  <-- Saved model
data/
  input.csv        <-- Sample rows
  predictions.csv  <-- Batch predictions output
batch_infer.py     <-- Batch inference runner
train_baseline.py  <-- Training script (tiny demo model)
Dockerfile
requirements.txt
README.md
screenshots/
  metrics.png
  terminal_output.png
```

## ğŸš€ Getting Started
1ï¸âƒ£ Create & activate your virtual environment
```
python -m venv venv
venv\Scripts\activate
```

2ï¸âƒ£ Install the dependencies
```
pip install -r requirements.txt
```

ğŸ¤– Train the Tiny Baseline Model

You only need to run this once:
```
python train_baseline.py
```


This creates:
```
models/baseline.joblib
```
(A tiny LogisticRegression model with 2 features â€” perfect for serving demos.)

âš¡ Run the API
```
uvicorn app.main:app --reload
```

Now your endpoints are live:
http://localhost:8000

ğŸ”¥ API Endpoints
âœ”ï¸ Health Check

GET /health

{"status": "ok"}

âœ”ï¸ Prediction

POST /predict

Input
{ "x1": 1.0, "x2": 2.0 }

Output
{
  "score": 1.0,
  "model_version": "v1.0"
}

Curl Example
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d "{\"x1\":1.0,\"x2\":2.0}"

âœ”ï¸ Prometheus Metrics

GET /metrics

Your metrics include:

request counter

request latency histogram

python process memory

GC stats

A screenshot is included:

screenshots/metrics.png

ğŸ“ Batch Inference

Run model predictions on a CSV file:

python batch_infer.py data/input.csv data/predictions.csv

Output:

A CSV with the original rows plus a prediction column.

ğŸ³ Docker Setup
Build the image
docker build -t model-server:v1 .

Run the container
docker run -p 8000:8000 model-server:v1


Now test inside Docker:

curl http://localhost:8000/health

ğŸ“Œ Assignment Checklist
Requirement	Status
FastAPI app works	âœ”ï¸
/predict returns score + version	âœ”ï¸
Batch inference script runs	âœ”ï¸
Prometheus metrics at /metrics	âœ”ï¸
Docker builds & serves app	âœ”ï¸
README covers everything	âœ”ï¸